{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.0 Install Necessary Packages"
      ],
      "metadata": {
        "id": "_9pXODj130IE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBLs2nL6Gerj",
        "outputId": "463ebec4-08a9-474b-dece-67d7781cae3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.330\n",
            "  Downloading langchain-0.0.330-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.330)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.330)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain==0.0.330)\n",
            "  Downloading langsmith-0.0.60-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.330) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.330) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.330) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.330) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.330) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.330)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.330)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.330)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.330) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.330) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.330) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.330) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.330) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.330)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.330 langsmith-0.0.60 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Install the langchain package\n",
        "!pip install langchain==0.0.330"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NESeJJZQGhUS",
        "outputId": "be3bc43f-1cea-445e-943e-427c092ab92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting typing_extensions==4.7.1\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: typing_extensions\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing_extensions-4.7.1\n"
          ]
        }
      ],
      "source": [
        "# Update the typing_extensions package\n",
        "!pip install typing_extensions==4.7.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMjUI-tq34h8",
        "outputId": "5cd9b8fc-dfaf-4c3e-967f-4a4e61c0f58a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.10.13-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-12.0 yt-dlp-2023.10.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install openai==0.28.1"
      ],
      "metadata": {
        "id": "ZJeV0Q3YFmpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.1 Imports and Initializations"
      ],
      "metadata": {
        "id": "dZDXYdKD35bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain as lc\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import openai"
      ],
      "metadata": {
        "id": "8-cIsLVN7PJQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = 'YOUR_KEY'\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "InYLir018RXv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "NW_A-9r8FuHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkON9gdTH6Uk",
        "outputId": "ab58cf31-0875-4c66-bf96-f6dbeb3881bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='A language model is like a clever computer program that can understand and generate human language. It can read a sentence or a paragraph and try to predict what comes next. It\\'s kind of like a brain that knows a lot of words and grammar rules, and it uses that knowledge to guess what words should come next in a sentence.\\n\\nFor example, if you start typing \"Once upon a time, there was a\", a language model might guess that the next word could be \"princess\" or \"dragon\" because those words often come after \"Once upon a time, there was a\" in fairy tales.\\n\\nLanguage models are used in many things we use every day, like voice assistants on our phones or smart speakers. They help these devices understand what we say and generate responses that sound like human language.\\n\\nScientists and engineers are always working to make language models better, so they can understand and generate more accurate and natural-sounding sentences. It\\'s really fascinating how these models can help us communicate with computers in a more human-like way.'\n"
          ]
        }
      ],
      "source": [
        "msgs_what_is_llm = [\n",
        "    SystemMessage(content=\"You are a machine learning expert who writes for an audience of 10 year olds.\"),\n",
        "    HumanMessage(content=\"What is a Language Model?\")\n",
        "]\n",
        "\n",
        "response_what_is_llm = chat(msgs_what_is_llm)\n",
        "\n",
        "print(response_what_is_llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VceLIpm4IRbJ",
        "outputId": "4a2adad3-00a1-4ea4-d008-4fbd9787863a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Language models are incredibly useful for various reasons. Here are a few ways they can be beneficial:\\n\\n1. Writing Assistance: Language models can help us with writing. They can suggest words or complete sentences as we type, making it easier and faster to express our thoughts. They can also help us check for grammar mistakes and improve the overall quality of our writing.\\n\\n2. Voice Assistants: Have you ever talked to a voice assistant like Siri or Alexa? They use language models to understand what we say and provide helpful responses. They can answer questions, set reminders, play music, and even tell jokes! Language models make these interactions feel more natural and conversational.\\n\\n3. Translation: Language models can assist in translating text from one language to another. They can analyze the context and grammar of a sentence to provide accurate translations. This is incredibly valuable for people who want to communicate with others who speak different languages.\\n\\n4. Chatbots: Have you ever chatted with a computer program that seems like a real person? These programs, called chatbots, use language models to understand our messages and generate responses. They can help answer questions, provide information, and even have fun conversations.\\n\\n5. Content Generation: Language models can generate text for various purposes, such as writing stories, creating news articles, or composing emails. They can be used to automate content creation and save time for writers and journalists.\\n\\nOverall, language models are useful because they make it easier for computers to understand and communicate with us in a more human-like way. They can assist us with writing, provide helpful information, and make our interactions with technology more seamless.'\n"
          ]
        }
      ],
      "source": [
        "msgs_what_is_llm_conversation = msgs_what_is_llm + [response_what_is_llm, HumanMessage(content=\"Why might that be useful?\")]\n",
        "\n",
        "response_what_is_llm_conversation = chat(msgs_what_is_llm_conversation)\n",
        "\n",
        "print(response_what_is_llm_conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.0 Load Youtube Audio"
      ],
      "metadata": {
        "id": "D20jrsgT4Pzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXT = 'm4a'\n",
        "FILE_NAME = 'myaudio'"
      ],
      "metadata": {
        "id": "pEhI-8yVFWpv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -x -f 233 -o {FILE_NAME} 'https://www.youtube.com/watch?v=fLvJ8VdHLA0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8geCpMjE4k_5",
        "outputId": "18bbb4c0-9a66-4656-c948-228f0fd20a8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=fLvJ8VdHLA0\n",
            "[youtube] fLvJ8VdHLA0: Downloading webpage\n",
            "[youtube] fLvJ8VdHLA0: Downloading ios player API JSON\n",
            "[youtube] fLvJ8VdHLA0: Downloading android player API JSON\n",
            "[youtube] fLvJ8VdHLA0: Downloading m3u8 information\n",
            "[info] fLvJ8VdHLA0: Downloading 1 format(s): 233\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 117\n",
            "[download] Destination: myaudio\n",
            "\u001b[K[download] 100% of    3.39MiB in \u001b[1;37m00:00:02\u001b[0m at \u001b[0;32m1.29MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: myaudio.m4a\n",
            "Deleting original file myaudio (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2r0wM7HXJnDv"
      },
      "outputs": [],
      "source": [
        "with open (f\"{FILE_NAME}.{EXT}\", \"rb\") as audio_file:\n",
        "  transcript = openai.Audio.transcribe(\n",
        "      file=audio_file,\n",
        "      model=\"whisper-1\",\n",
        "      response_format=\"text\",\n",
        "      language=\"en\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uajJeXVKP2F",
        "outputId": "a1712e6a-719d-4ce2-beb7-d10c3ff89501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is natural language processing? Well, you're doing it right now. You're listening to the words and the sentences that I'm forming, and you are forming some sort of comprehension from it. And when we ask a computer to do that, that is NLP, or natural language processing. My name is Martin Keane. I'm a master inventor at IBM, and I've utilised NLP in a good number of my invention disclosures. NLP really has a really high utility value in all sorts of AI applications. Now, NLP starts with something called unstructured text. What is that? Well, that's just what you and I say. That's how we speak. So, for example, some unstructured text is, add eggs and milk to my shopping list. Now, you and I understand exactly what that means, but it is unstructured, at least to a computer. So, what we need to do is to have a structured representation of that same information that a computer can process. Now, that might look something a bit more like this, where we have a shopping list element, and then it has sub-elements within it, like an item for eggs and an item for milk. That is an example of something that is structured. Now, the job of natural language processing is to translate between these two things. So, NLP sits right in the middle here, translating between unstructured and structured data. And when we go from unstructured here to structured this way, that's called NLU, or Natural Language Understanding. And when we go this way, from structured to unstructured, that's called Natural Language Generation, or NLG. We're going to focus today primarily on going from unstructured to structured in natural language processing. Now, let's think of some use cases where NLP might be quite handy. First of all, we've got machine translation. Now, when we translate from one language to another, we need to understand the context of that sentence. It's not just a case of taking each individual word from, say, English, and then translating it into another language. We need to understand the overall structure and context of what's being said. And my favorite example of this going horribly wrong is if you take the phrase, the spirit is willing, but the flesh is weak, and you translate that from English to Russian, and then you translate that Russian translation back into English, you're going to go from the spirit is willing, but the flesh is weak, to something a bit more like the vodka is good, but the meat is rotten, which is really not the intended context of that sentence whatsoever. So NLP can help with situations like that. Now, the second kind of use case that I like to mention relates to virtual assistants and also to things like chatbots. Now, a virtual assistant, that's something like Siri or Alexa on your phone, that is taking human utterances and deriving a command to execute based upon that. And a chatbot is something similar except in written language, and that's taking written language and then using it to traverse a decision tree in order to take an action. NLP is very helpful there. Another use case is for sentiment analysis. Now, this is taking some text, perhaps an email message or a product review, and trying to derive the sentiment that is expressed within it. So, for example, is this product review a positive sentiment or a negative sentiment? Is it written as a serious statement or is it being sarcastic? We can use NLP to tell us. And then finally, another good example is spam detection. So this is a case of looking at a given email message and trying to derive, is this a real email message or is it spam? And we can look for pointers within the content of the message. So things like overused words or poor grammar or an inappropriate claim of urgency can all indicate that this is actually perhaps spam. So those are some of the things that NLP can provide, but how does it work? Well, the thing with NLP is it's not like one algorithm. It's actually more like a bag of tools, and you can apply these bag of tools to be able to resolve some of these use cases. Now, the input to NLP is some unstructured text, so either some written text or spoken text that has been converted to written text through a speech-to-text algorithm. Once we've got that, the first stage of NLP is called tokenization. This is about taking a string and breaking it down into chunks. So if we consider the unstructured text we've got here, add eggs and milk to my shopping list, that's eight words, that could be eight tokens. And from here on in we are going to work one token at a time as we traverse through this. Now the first stage, once we've got things down into tokens that we can perform, is called stemming. And this is all about deriving the word stem for a given token. So for example, running, runs, and ran, the word stem for all three of those is run. We're just kind of removing the prefix and the suffixes and normalizing the tense, and we're getting to the word stem. But stemming doesn't work well for every token. For example, universal and university, well they don't really stem down to universe. For situations like that there is another tool that we have available, and that is called lemmatization. And lemmatization takes a given token and learns its meaning through a dictionary definition, and from there it can derive its root or its lem. So take better for example. Better is derived from good, so the root or the lem of better is good. The stem of better would be bet. So you can see that it is significant whether we use stemming or we use lemmatization for a given token. Now next thing we can do is we can do a process called part of speech tagging. And what this is doing is, for a given token, it's looking where that token is used within the context of a sentence. So take the word make for example. If I say I'm going to make dinner, make is a verb. But if I ask you what make is your laptop, well make is now a noun. So where that token is used in the sentence matters. Part of speech tagging can help us derive that context. And then finally another stage is named entity recognition. And what this is asking is, for a given token, is there an entity associated with it? So for example, a token of Arizona has an entity of a US state, whereas a token of Ralph has an entity of a person's name. And these are some of the tools that we can apply in this big bag of tools that we have for NLP in order to get from this unstructured human speech through to something structured that a computer can understand. And once we've done that, then we can apply that structured data to all sorts of AI applications. Now there's obviously a lot more to it than this, and I've included some links in the description if you'd like to know more. But hopefully this made some sense and that you were able to process some of the natural language that I've shared today. Thanks for watching. If you have questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K5JoFfGZKRCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f202ab5f-3ea6-4c92-fa66-7b6a76997cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Summary:\\n\\nIntroduction to Natural Language Processing (NLP)\\n- NLP is the process of enabling computers to understand and process human language.\\n- It involves translating between unstructured text (how humans speak) and structured data (computer-readable format).\\n\\nNLP Use Cases\\n1. Machine translation: Translating sentences while considering context.\\n2. Virtual assistants and chatbots: Understanding human commands and generating appropriate responses.\\n3. Sentiment analysis: Determining the sentiment expressed in text (positive, negative, sarcastic, etc.).\\n4. Spam detection: Identifying spam emails based on content analysis.\\n\\nHow NLP Works\\n- NLP is a collection of tools and algorithms applied to unstructured text.\\n- The process starts with tokenization, breaking down the text into individual chunks (tokens).\\n- Stemming and lemmatization are used to normalize tokens by deriving their word stems or roots.\\n- Part of speech tagging helps determine the context of tokens based on their usage in a sentence.\\n- Named entity recognition identifies entities associated with tokens (e.g., names, locations).\\n\\nApplying Structured Data\\n- Once the unstructured text is transformed into structured data, it can be used in various AI applications.\\n\\nConclusion and Additional Resources\\n- NLP is a complex process with many tools and techniques.\\n- Further resources are provided in the video description for more information.'\n"
          ]
        }
      ],
      "source": [
        "msgs_summarize = [\n",
        "    SystemMessage(content=\"You are a teacher who writes study guides.\"),\n",
        "    HumanMessage(content=f\"Write a formatted summary with headings of the following transcript.\\n\\n{transcript}\")\n",
        "]\n",
        "\n",
        "response_summarize = chat(msgs_summarize)\n",
        "\n",
        "print(response_summarize)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{response_summarize.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3ftWTceAbKz",
        "outputId": "8e237201-022c-4ae0-fcbb-2df85b45ef75"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "\n",
            "Introduction to Natural Language Processing (NLP)\n",
            "- NLP is the process of enabling computers to understand and process human language.\n",
            "- It involves translating between unstructured text (how humans speak) and structured data (computer-readable format).\n",
            "\n",
            "NLP Use Cases\n",
            "1. Machine translation: Translating sentences while considering context.\n",
            "2. Virtual assistants and chatbots: Understanding human commands and generating appropriate responses.\n",
            "3. Sentiment analysis: Determining the sentiment expressed in text (positive, negative, sarcastic, etc.).\n",
            "4. Spam detection: Identifying spam emails based on content analysis.\n",
            "\n",
            "How NLP Works\n",
            "- NLP is a collection of tools and algorithms applied to unstructured text.\n",
            "- The process starts with tokenization, breaking down the text into individual chunks (tokens).\n",
            "- Stemming and lemmatization are used to normalize tokens by deriving their word stems or roots.\n",
            "- Part of speech tagging helps determine the context of tokens based on their usage in a sentence.\n",
            "- Named entity recognition identifies entities associated with tokens (e.g., names, locations).\n",
            "\n",
            "Applying Structured Data\n",
            "- Once the unstructured text is transformed into structured data, it can be used in various AI applications.\n",
            "\n",
            "Conclusion and Additional Resources\n",
            "- NLP is a complex process with many tools and techniques.\n",
            "- Further resources are provided in the video description for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msgs_key_moments = msgs_summarize + [response_summarize, HumanMessage(content=\"What is tokenization as described in the transcript? Share a snippet of the transcript where it is explained.\")]\n",
        "respone_key_moments = chat(msgs_key_moments)\n",
        "\n",
        "print(f\"{respone_key_moments.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRpm2QNiAkVj",
        "outputId": "13dd5bc0-3cf5-42a5-8449-d5824f4ce03e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization, as described in the transcript, is the process of breaking down a string of text into individual chunks or tokens. Here is the snippet of the transcript where tokenization is explained:\n",
            "\n",
            "\"Now the first stage of NLP is called tokenization. This is about taking a string and breaking it down into chunks. So if we consider the unstructured text we've got here, add eggs and milk to my shopping list, that's eight words, that could be eight tokens. And from here on in we are going to work one token at a time as we traverse through this.\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}